---
description: Universal Python and Data Engineering standards for production-grade code
globs: ["*.py", "**/*.py"]
alwaysApply: true
---

# Python & Data Engineering Standards

> Universal rules for production-grade Python code across all projects.
> Aligned with industry best practices and career progression from Data Analyst â†’ LLM Engineer.

---

## ğŸ¯ Code Philosophy

### Guiding Principles
1. **Readability over cleverness** â€” Code is read 10x more than written
2. **Explicit over implicit** â€” No magic; make intentions clear
3. **Fail fast, fail loud** â€” Catch errors early with meaningful messages
4. **Test what matters** â€” Coverage is a tool, not a goal
5. **Document for your future self** â€” You won't remember in 6 months

---

## ğŸ“ Code Style

### Naming Conventions
```python
# Variables and functions: snake_case
user_count = 0
def calculate_total_amount():
    pass

# Classes: PascalCase
class DataProcessor:
    pass

# Constants: SCREAMING_SNAKE_CASE
MAX_RETRY_ATTEMPTS = 3
DEFAULT_TIMEOUT_SECONDS = 30

# Private methods/attributes: single leading underscore
def _internal_helper():
    pass

# "Dunder" methods: double underscore (Python reserved)
def __init__(self):
    pass
```

### Formatting Standards
- **Line length:** 88 characters (Black formatter default)
- **Indentation:** 4 spaces (never tabs)
- **Imports:** Group in order: stdlib â†’ third-party â†’ local, alphabetized within groups
- **Blank lines:** 2 between top-level definitions, 1 between methods
- **Trailing commas:** Always in multi-line structures (enables cleaner diffs)

### Import Organization
```python
# Standard library
import logging
import os
from datetime import datetime
from pathlib import Path

# Third-party packages
import pandas as pd
import numpy as np
from pydantic import BaseModel

# Local application imports
from src.config import settings
from src.utils.validators import validate_input
```

---

## ğŸ”¤ Type Hints (Required)

### Function Signatures
```python
# âœ… Good: Full type hints with return type
def process_records(
    df: pd.DataFrame,
    column_name: str,
    threshold: float = 0.5,
) -> pd.DataFrame:
    """Process records based on threshold."""
    pass

# âŒ Bad: No type hints
def process_records(df, column_name, threshold=0.5):
    pass
```

### Common Type Patterns
```python
from typing import Optional, Union, List, Dict, Any, Callable, Tuple

# Optional (can be None)
def find_user(user_id: str) -> Optional[dict]:
    pass

# Union (multiple types)
def parse_value(value: Union[str, int]) -> float:
    pass

# Collections with element types
def get_names() -> List[str]:
    pass

def get_config() -> Dict[str, Any]:
    pass

# Callable (function as parameter)
def apply_transform(data: pd.DataFrame, func: Callable[[pd.Series], pd.Series]) -> pd.DataFrame:
    pass
```

---

## ğŸ“ Docstrings (NumPy Style)

### Function Docstring Template
```python
def reconcile_amounts(
    source_df: pd.DataFrame,
    target_df: pd.DataFrame,
    tolerance: float = 0.50,
) -> pd.DataFrame:
    """Reconcile amounts between source and target DataFrames.

    Compares amounts from two sources and flags discrepancies
    that exceed the specified tolerance threshold.

    Parameters
    ----------
    source_df : pd.DataFrame
        DataFrame containing source system amounts.
        Must have columns: ['id', 'amount', 'date'].
    target_df : pd.DataFrame
        DataFrame containing target system amounts.
        Must have columns: ['id', 'amount', 'date'].
    tolerance : float, optional
        Maximum allowed difference for matching.
        Defaults to 0.50.

    Returns
    -------
    pd.DataFrame
        DataFrame with reconciliation results including:
        - match_status: 'MATCHED' | 'DISCREPANCY' | 'MISSING'
        - difference: Calculated amount difference
        - action: Recommended action for review

    Raises
    ------
    ValueError
        If required columns are missing from input DataFrames.
    TypeError
        If tolerance is not a numeric value.

    Examples
    --------
    >>> source = pd.DataFrame({'id': [1, 2], 'amount': [100.0, 200.0]})
    >>> target = pd.DataFrame({'id': [1, 2], 'amount': [100.0, 199.0]})
    >>> result = reconcile_amounts(source, target, tolerance=0.50)
    >>> result['match_status'].tolist()
    ['MATCHED', 'DISCREPANCY']
    """
    pass
```

### Class Docstring Template
```python
class DataPipeline:
    """Orchestrates data extraction, transformation, and loading.

    This class manages the end-to-end data pipeline workflow,
    handling configuration, validation, and error recovery.

    Attributes
    ----------
    config : dict
        Pipeline configuration settings.
    logger : logging.Logger
        Configured logging instance.
    is_initialized : bool
        Whether pipeline has been set up.

    Examples
    --------
    >>> pipeline = DataPipeline(config={'source': 'db'})
    >>> pipeline.run()
    >>> print(pipeline.status)
    'completed'
    """
    pass
```

---

## ğŸ¼ pandas Best Practices

### DataFrame Operations
```python
# âœ… Always use .copy() when creating derived DataFrames
filtered_df = original_df[original_df['status'] == 'active'].copy()

# âœ… Use .loc[] for assignment (avoids SettingWithCopyWarning)
df.loc[df['amount'] < 0, 'flag'] = 'NEGATIVE'

# âŒ Avoid chained indexing
df[df['amount'] < 0]['flag'] = 'NEGATIVE'  # May not work!

# âœ… Use nullable dtypes for data with missing values
df['count'] = df['count'].astype('Int64')      # Nullable integer
df['amount'] = df['amount'].astype('Float64')  # Nullable float

# âœ… Use pd.NA for missing values (not np.nan for nullable dtypes)
df.loc[mask, 'value'] = pd.NA

# âœ… Prefer vectorized operations over .apply()
# Good:
df['total'] = df['price'] * df['quantity']
# Slower:
df['total'] = df.apply(lambda row: row['price'] * row['quantity'], axis=1)

# âœ… Use .query() for complex filtering (more readable)
result = df.query('amount > 100 and status == "active"')
```

### Memory and Performance
```python
# âœ… Specify dtypes when reading data
df = pd.read_csv(
    'data.csv',
    dtype={
        'id': 'str',
        'amount': 'Float64',
        'category': 'category',  # For repeated strings
    },
    parse_dates=['created_at'],
)

# âœ… Use categories for repeated string values
df['status'] = df['status'].astype('category')

# âœ… Select only needed columns early
df = pd.read_csv('large_file.csv', usecols=['id', 'amount', 'date'])
```

### Merging and Joining
```python
# âœ… Always specify merge type explicitly
result = pd.merge(
    left_df,
    right_df,
    on='key_column',
    how='left',           # Be explicit: 'left', 'right', 'inner', 'outer'
    validate='m:1',       # Validate relationship: '1:1', '1:m', 'm:1', 'm:m'
    indicator=True,       # Adds _merge column showing match status
)

# âœ… Check for unexpected duplicates after merge
assert result.duplicated(subset=['id']).sum() == 0, "Unexpected duplicates!"
```

---

## âš ï¸ Error Handling

### Pydantic for Structured Outputs & Validation

```python
from pydantic import BaseModel, Field
from typing import Optional
from enum import Enum

# âœ… Define response schemas for LLM structured outputs
class RiskLevel(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"

class DemandInsight(BaseModel):
    """Structured output schema for AI-generated demand analysis."""

    summary: str = Field(
        ...,
        description="One-sentence summary of the demand pattern",
        max_length=200,
    )
    trend_direction: str = Field(
        ...,
        description="Direction of demand trend",
        pattern=r"^(increasing|decreasing|stable)$",
    )
    confidence: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Model confidence in the analysis (0-1)",
    )
    risk_level: RiskLevel = Field(
        ...,
        description="Risk level for capacity planning",
    )
    recommended_action: Optional[str] = Field(
        default=None,
        description="Suggested next step for operations team",
    )

# âœ… Validate AI responses â€” reject malformed outputs
try:
    insight = DemandInsight.model_validate(ai_response)
except ValidationError as e:
    logger.error(f"AI response failed validation: {e}")
    # Fallback or retry logic here

# âœ… Use for config validation too
class PipelineConfig(BaseModel):
    """Validated configuration â€” catches errors at startup, not runtime."""

    source_path: Path
    output_dir: Path
    tolerance: float = Field(default=0.50, ge=0.0, le=1.0)
    max_retries: int = Field(default=3, ge=1, le=10)

    class Config:
        frozen = True  # Immutable after creation
```

### Pydantic Best Practices
```python
# âœ… Use Field() with descriptions â€” doubles as LLM schema documentation
# âœ… Use validators (ge, le, pattern, max_length) â€” catch bad data early
# âœ… Use Enum types for constrained string fields
# âœ… Use frozen=True for configs that shouldn't change after creation
# âœ… Use model_validate() not __init__() for external data (API responses, files)

# âŒ Don't use plain dicts where Pydantic models would add safety
# âŒ Don't skip Field descriptions â€” they feed into LLM JSON schemas
# âŒ Don't catch ValidationError silently â€” log and handle explicitly
```

---

### Exception Patterns
```python
# âœ… Create custom exceptions for domain-specific errors
class ValidationError(Exception):
    """Raised when data validation fails."""
    pass

class ConfigurationError(Exception):
    """Raised when configuration is invalid or missing."""
    pass

# âœ… Raise early with context
def process_file(filepath: Path) -> pd.DataFrame:
    if not filepath.exists():
        raise FileNotFoundError(f"Input file not found: {filepath}")
    
    if filepath.suffix not in ['.csv', '.xlsx']:
        raise ValueError(f"Unsupported file type: {filepath.suffix}")
    
    # Happy path continues...

# âœ… Catch specific exceptions, not bare except
try:
    result = risky_operation()
except ValueError as e:
    logger.warning(f"Invalid value encountered: {e}")
    result = default_value
except FileNotFoundError as e:
    logger.error(f"Required file missing: {e}")
    raise  # Re-raise if unrecoverable

# âŒ Never use bare except
try:
    result = something()
except:  # Catches EVERYTHING including KeyboardInterrupt!
    pass
```

### Guard Clauses (Early Returns)
```python
# âœ… Good: Guard clauses reduce nesting
def calculate_bonus(employee: dict) -> float:
    if employee is None:
        return 0.0
    
    if employee.get('status') != 'active':
        return 0.0
    
    if employee.get('tenure_years', 0) < 1:
        return 0.0
    
    # Happy path: calculate actual bonus
    base_salary = employee['salary']
    return base_salary * 0.10

# âŒ Bad: Deeply nested conditionals
def calculate_bonus(employee: dict) -> float:
    if employee is not None:
        if employee.get('status') == 'active':
            if employee.get('tenure_years', 0) >= 1:
                base_salary = employee['salary']
                return base_salary * 0.10
    return 0.0
```

---

## âš¡ Async Patterns (API Calls & Data Collection)

### When to Use Async
```python
# Use async when:
# - Making multiple API calls (LLM SDKs, REST APIs, data collectors)
# - I/O-bound operations that can run concurrently
# - Building Streamlit apps that call external services

# Do NOT use async when:
# - CPU-bound operations (use multiprocessing instead)
# - Simple scripts with a single sequential flow
# - You don't understand the pattern yet (learn sync first, then async)
```

### Basic Async Pattern with httpx
```python
import httpx
import asyncio

# âœ… Async HTTP client for API calls
async def fetch_market_data(
    symbols: list[str],
    client: httpx.AsyncClient,
) -> dict[str, dict]:
    """Fetch market data for multiple symbols concurrently.

    Parameters
    ----------
    symbols : list[str]
        Stock ticker symbols to fetch.
    client : httpx.AsyncClient
        Shared async HTTP client (reuse for connection pooling).

    Returns
    -------
    dict[str, dict]
        Symbol-keyed dictionary of market data responses.
    """
    tasks = [client.get(f"/api/quote/{s}") for s in symbols]
    responses = await asyncio.gather(*tasks, return_exceptions=True)

    results = {}
    for symbol, response in zip(symbols, responses):
        if isinstance(response, Exception):
            logger.error(f"Failed to fetch {symbol}: {response}")
            continue
        results[symbol] = response.json()
    return results

# âœ… Always use async context manager for clients
async def main():
    async with httpx.AsyncClient(base_url="https://api.example.com") as client:
        data = await fetch_market_data(["AAPL", "MSFT", "GOOGL"], client)
```

### Async Best Practices
```python
# âœ… Use asyncio.gather() for concurrent calls (not sequential awaits)
# âœ… Use return_exceptions=True to prevent one failure from killing all tasks
# âœ… Use httpx.AsyncClient as context manager (auto-closes connections)
# âœ… Add timeouts: httpx.AsyncClient(timeout=30.0)

# âŒ Don't mix sync and async in the same function
# âŒ Don't create a new client per request (connection pooling matters)
# âŒ Don't use requests library in async code (use httpx instead)
```

---

### Logging Setup
```python
import logging

# Configure at module level
logger = logging.getLogger(__name__)

# In main/entry point, configure the root logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
)
```

### Logging Levels
```python
# DEBUG: Detailed diagnostic information
logger.debug(f"Processing row {idx}: {row.to_dict()}")

# INFO: Confirmation that things work as expected
logger.info(f"Loaded {len(df)} records from {filepath}")

# WARNING: Something unexpected but code can continue
logger.warning(f"Missing optional column 'notes', using default")

# ERROR: Serious problem, some functionality failed
logger.error(f"Failed to process file {filepath}: {e}")

# CRITICAL: Program cannot continue
logger.critical(f"Database connection failed, shutting down")
```

### Structured Logging for Production
```python
# âœ… Include context in log messages
logger.info(
    "Processing complete",
    extra={
        'records_processed': len(df),
        'duration_seconds': elapsed,
        'output_file': str(output_path),
    }
)
```

---

## ğŸ§ª Testing Standards

### Test Structure
```python
import pytest
import pandas as pd
from src.module import function_to_test

class TestFunctionName:
    """Tests for function_to_test."""

    def test_happy_path(self):
        """Should return expected result for valid input."""
        result = function_to_test(valid_input)
        assert result == expected_output

    def test_empty_input(self):
        """Should handle empty input gracefully."""
        result = function_to_test([])
        assert result == []

    def test_invalid_input_raises(self):
        """Should raise ValueError for invalid input."""
        with pytest.raises(ValueError, match="Invalid"):
            function_to_test(invalid_input)

    @pytest.mark.parametrize("input,expected", [
        (1, 2),
        (2, 4),
        (0, 0),
    ])
    def test_multiple_cases(self, input, expected):
        """Should handle various inputs correctly."""
        assert function_to_test(input) == expected
```

### DataFrame Testing
```python
import pandas.testing as tm

def test_dataframe_transformation():
    """Should transform DataFrame correctly."""
    input_df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
    expected_df = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [4, 6]})
    
    result = transform_function(input_df)
    
    tm.assert_frame_equal(result, expected_df)
```

### Fixtures for Reusable Test Data
```python
@pytest.fixture
def sample_dataframe():
    """Create sample DataFrame for testing."""
    return pd.DataFrame({
        'id': ['A001', 'A002', 'A003'],
        'amount': [100.0, 200.0, 300.0],
        'status': ['active', 'active', 'inactive'],
    })

def test_filter_active(sample_dataframe):
    """Should filter to active records only."""
    result = filter_active(sample_dataframe)
    assert len(result) == 2
```

---

## ğŸ”’ Security & Privacy

### Never Expose Sensitive Data
```python
# âœ… Mask sensitive values in logs and debug output
def mask_ssn(ssn: str) -> str:
    """Mask SSN for safe logging: 123-45-6789 â†’ ***-**-6789"""
    if len(ssn) >= 4:
        return f"***-**-{ssn[-4:]}"
    return "***"

# âœ… Use environment variables for secrets
import os
api_key = os.environ.get('API_KEY')

# âŒ Never hardcode secrets
api_key = "sk-abc123..."  # NEVER DO THIS
```

### Safe File Handling
```python
# âœ… Use pathlib for cross-platform paths
from pathlib import Path

data_dir = Path(__file__).parent / 'data'
output_file = data_dir / 'results.csv'

# âœ… Use context managers for file operations
with open(filepath, 'r') as f:
    content = f.read()
```

---

## ğŸ“ Project Structure (Recommended)
```
project_name/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/          # Configuration and settings
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py
â”‚   â”œâ”€â”€ core/            # Core business logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ processor.py
â”‚   â”œâ”€â”€ ai/              # LLM SDK integration (provider-agnostic)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ provider.py      # SDK abstraction (Gemini primary, fallbacks)
â”‚   â”‚   â”œâ”€â”€ schemas.py       # Pydantic models for structured outputs
â”‚   â”‚   â”œâ”€â”€ guardrails.py    # Query validation, response sanitization
â”‚   â”‚   â””â”€â”€ observability.py # Token/cost/latency logging
â”‚   â””â”€â”€ utils/           # Shared utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ validators.py
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py      # Shared fixtures
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample/          # Sample/synthetic data only
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ logs/                # Runtime logs (gitignored)
â”‚   â””â”€â”€ ai/              # AI-specific logs (queries, guardrails, costs)
â”œâ”€â”€ notebooks/           # Exploration and documentation
â”œâ”€â”€ docs/
â”œâ”€â”€ .cursorrules
â”œâ”€â”€ .cursor/rules/
â”œâ”€â”€ pyproject.toml       # Project config (preferred over setup.py)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ–¥ï¸ Streamlit Conventions (Dashboard & AI UI)

### App Structure
```python
# âœ… Multi-page app structure (recommended for dashboards)
# project/
# â”œâ”€â”€ app.py                  # Entry point with st.navigation
# â”œâ”€â”€ pages/
# â”‚   â”œâ”€â”€ 01_Overview.py
# â”‚   â”œâ”€â”€ 02_Analysis.py
# â”‚   â”œâ”€â”€ 03_AI_Chat.py
# â”‚   â””â”€â”€ 04_Settings.py
# â””â”€â”€ src/                    # Business logic (NOT in pages/)
```

### Caching Strategy
```python
import streamlit as st

# âœ… Cache data loading (returns new copy each call â€” safe for DataFrames)
@st.cache_data(ttl=3600)  # Cache for 1 hour
def load_data(filepath: str) -> pd.DataFrame:
    """Load and cache data. TTL prevents stale data."""
    return pd.read_parquet(filepath)

# âœ… Cache resource objects (returns SAME object â€” use for DB connections, clients)
@st.cache_resource
def get_llm_client() -> object:
    """Create and cache LLM client. Shared across all users/reruns."""
    return genai.GenerativeModel("gemini-2.0-flash")

# âŒ Don't cache functions with side effects (logging, writes, API mutations)
# âŒ Don't use @st.cache_data for unhashable objects (DB connections, clients)
# âŒ Don't forget TTL on data caches â€” stale data causes confusion
```

### Session State for AI Interactions
```python
# âœ… Initialize state once, use everywhere
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
    st.session_state.total_tokens = 0
    st.session_state.total_cost = 0.0

# âœ… Append AI responses to session state for conversation memory
st.session_state.chat_history.append({
    "role": "assistant",
    "content": response.text,
    "tokens": response.usage_metadata.total_token_count,
})

# âŒ Don't store large DataFrames in session state (use @st.cache_data instead)
# âŒ Don't assume session state persists across browser tabs
```

### Streamlit + LLM SDK Pattern
```python
# âœ… Keep business logic out of Streamlit pages
# pages/03_AI_Chat.py calls src/ai/provider.py â€” never has SDK logic inline

# âœ… Show AI observability to users
st.sidebar.metric("Tokens Used", st.session_state.total_tokens)
st.sidebar.metric("Est. Cost", f"${st.session_state.total_cost:.4f}")

# âœ… Always show disclaimers for AI-generated content
st.caption("âš ï¸ AI-generated insight. Verify before acting.")
```

---

## ğŸš« Anti-Patterns to Avoid
```python
# âŒ Mutable default arguments
def append_item(item, items=[]):  # BUG: list persists between calls!
    items.append(item)
    return items

# âœ… Use None and create inside function
def append_item(item, items=None):
    if items is None:
        items = []
    items.append(item)
    return items

# âŒ Using == for None comparison
if value == None:
    pass

# âœ… Use 'is' for None
if value is None:
    pass

# âŒ Catching and silencing all errors
try:
    risky_thing()
except:
    pass  # Silent failure hides bugs

# âŒ String concatenation in loops (inefficient)
result = ""
for item in items:
    result += str(item)  # Creates new string each time

# âœ… Use join
result = "".join(str(item) for item in items)
```

---

## âœ… Quality Checklist (Before Every Commit)

- [ ] All functions have type hints
- [ ] All public functions have NumPy-style docstrings
- [ ] No `print()` statements (use logging)
- [ ] No hardcoded secrets or sensitive data
- [ ] Pydantic models validate all external data (API responses, configs, file inputs)
- [ ] Tests pass: `pytest tests/ -v`
- [ ] Code formatted: `black src/ tests/`
- [ ] Imports sorted: `isort src/ tests/`
- [ ] No linting errors: `ruff check src/`
- [ ] AI responses validated through Pydantic schemas (never trust raw LLM output)